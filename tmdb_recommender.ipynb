{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# H·ªá th·ªëng g·ª£i √Ω phim v·ªõi TMDB 5000\n",
        "\n",
        "**M√¥n**: Final Project - Recommendation System  \n",
        "**M√¥ t·∫£**: X√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω phim (movie recommender) d·ª±a tr√™n d·ªØ li·ªáu TMDB 5000 (`tmdb_5000_movies.csv`, `tmdb_5000_credits.csv`).\n",
        "\n",
        "C√°c ph·∫ßn ch√≠nh:\n",
        "1. Thu th·∫≠p & n·∫°p d·ªØ li·ªáu\n",
        "2. L√†m s·∫°ch & chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "3. Ph√¢n t√≠ch & tr·ª±c quan h√≥a d·ªØ li·ªáu\n",
        "4. X√¢y d·ª±ng h·ªá g·ª£i √Ω (content-based)\n",
        "5. ƒê√°nh gi√° m√¥ h√¨nh (RMSE, MAE, Precision@K, Recall@K)\n",
        "6. Giao di·ªán g·ª£i √Ω trong notebook (nh·∫≠p t√™n phim ƒë·ªÉ nh·∫≠n g·ª£i √Ω)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t v√† import th∆∞ vi·ªán\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "print(\"Th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Thu th·∫≠p & n·∫°p d·ªØ li·ªáu\n",
        "\n",
        "movies_path = 'tmdb_5000_movies.csv'\n",
        "credits_path = 'tmdb_5000_credits.csv'\n",
        "\n",
        "movies = pd.read_csv(movies_path)\n",
        "credits = pd.read_csv(credits_path)\n",
        "\n",
        "print('K√≠ch th∆∞·ªõc movies:', movies.shape)\n",
        "print('K√≠ch th∆∞·ªõc credits:', credits.shape)\n",
        "\n",
        "movies.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. L√†m s·∫°ch & chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "\n",
        "# G·ªôp th√¥ng tin credits v√†o movies theo movie_id\n",
        "credits_renamed = credits.rename(columns={'movie_id': 'id'})\n",
        "movies_merged = movies.merge(credits_renamed[['id', 'cast', 'crew']], on='id', how='left')\n",
        "\n",
        "print('K√≠ch th∆∞·ªõc sau khi merge:', movies_merged.shape)\n",
        "\n",
        "# Lo·∫°i b·ªè duplicate theo title\n",
        "before_dups = movies_merged.shape[0]\n",
        "movies_merged = movies_merged.drop_duplicates(subset=['title'])\n",
        "after_dups = movies_merged.shape[0]\n",
        "print(f'Drop duplicate theo title: {before_dups} -> {after_dups}')\n",
        "\n",
        "# X·ª≠ l√Ω missing values cho text: thay b·∫±ng chu·ªói r·ªóng\n",
        "text_cols = ['overview', 'tagline', 'cast', 'crew', 'keywords', 'genres']\n",
        "for col in text_cols:\n",
        "    if col in movies_merged.columns:\n",
        "        movies_merged[col] = movies_merged[col].fillna('')\n",
        "\n",
        "# X·ª≠ l√Ω missing cho numeric: thay b·∫±ng median\n",
        "num_cols = ['vote_average', 'vote_count', 'popularity', 'runtime']\n",
        "for col in num_cols:\n",
        "    if col in movies_merged.columns:\n",
        "        movies_merged[col] = movies_merged[col].fillna(movies_merged[col].median())\n",
        "\n",
        "# X·ª≠ l√Ω outlier ƒë∆°n gi·∫£n: clip vote_count ·ªü percentiles 1% - 99%\n",
        "low, high = movies_merged['vote_count'].quantile([0.01, 0.99])\n",
        "movies_merged['vote_count_clipped'] = movies_merged['vote_count'].clip(lower=low, upper=high)\n",
        "\n",
        "# Chu·∫©n h√≥a m·ªôt s·ªë ƒë·∫∑c tr∆∞ng numeric\n",
        "scaler = MinMaxScaler()\n",
        "movies_merged[['vote_avg_scaled', 'popularity_scaled', 'vote_count_scaled']] = scaler.fit_transform(\n",
        "    movies_merged[['vote_average', 'popularity', 'vote_count_clipped']]\n",
        ")\n",
        "\n",
        "movies_merged[['title', 'vote_average', 'vote_avg_scaled']].head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ph√¢n t√≠ch & Tr·ª±c quan h√≥a d·ªØ li·ªáu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.1. Ph√¢n b·ªë Rating (vote_average)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "ax[0].hist(movies_merged['vote_average'], bins=30, edgecolor='black', alpha=0.7)\n",
        "ax[0].set_xlabel('Vote Average')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_title('Ph√¢n b·ªë ƒëi·ªÉm ƒë√°nh gi√° phim')\n",
        "ax[0].grid(alpha=0.3)\n",
        "\n",
        "# Boxplot\n",
        "ax[1].boxplot(movies_merged['vote_average'], vert=True)\n",
        "ax[1].set_ylabel('Vote Average')\n",
        "ax[1].set_title('Boxplot c·ªßa Vote Average')\n",
        "ax[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean vote: {movies_merged['vote_average'].mean():.2f}\")\n",
        "print(f\"Median vote: {movies_merged['vote_average'].median():.2f}\")\n",
        "print(f\"Std vote: {movies_merged['vote_average'].std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.2. T·∫ßn su·∫•t th·ªÉ lo·∫°i phim (Genres)\n",
        "\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Parse genres t·ª´ JSON string\n",
        "def extract_genres(genres_str):\n",
        "    try:\n",
        "        genres_list = json.loads(genres_str)\n",
        "        return [g['name'] for g in genres_list]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "movies_merged['genres_list'] = movies_merged['genres'].apply(extract_genres)\n",
        "\n",
        "# ƒê·∫øm t·∫ßn su·∫•t\n",
        "all_genres = []\n",
        "for genres in movies_merged['genres_list']:\n",
        "    all_genres.extend(genres)\n",
        "\n",
        "genre_counts = Counter(all_genres)\n",
        "top_genres = genre_counts.most_common(15)\n",
        "\n",
        "# V·∫Ω bar chart\n",
        "genres_df = pd.DataFrame(top_genres, columns=['Genre', 'Count'])\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(genres_df['Genre'], genres_df['Count'], color='steelblue')\n",
        "plt.xlabel('S·ªë l∆∞·ª£ng phim')\n",
        "plt.ylabel('Th·ªÉ lo·∫°i')\n",
        "plt.title('Top 15 th·ªÉ lo·∫°i phim ph·ªï bi·∫øn nh·∫•t')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"T·ªïng s·ªë th·ªÉ lo·∫°i kh√°c nhau: {len(genre_counts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.3. Top 10 phim c√≥ rating cao nh·∫•t\n",
        "\n",
        "top_rated = movies_merged.nlargest(10, 'vote_average')[['title', 'vote_average', 'vote_count', 'popularity']]\n",
        "print(top_rated.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.4. Heatmap t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn s·ªë\n",
        "\n",
        "corr_cols = ['vote_average', 'vote_count', 'popularity', 'runtime', 'budget', 'revenue']\n",
        "corr_data = movies_merged[corr_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_data, annot=True, fmt='.2f', cmap='coolwarm', square=True, linewidths=0.5)\n",
        "plt.title('Heatmap t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn s·ªë')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. X√¢y d·ª±ng h·ªá g·ª£i √Ω phim (Content-Based Filtering)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.1. Chu·∫©n b·ªã features cho m√¥ h√¨nh\n",
        "\n",
        "# H√†m parse keywords\n",
        "def extract_keywords(keywords_str):\n",
        "    try:\n",
        "        keywords_list = json.loads(keywords_str)\n",
        "        return ' '.join([k['name'] for k in keywords_list])\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "# H√†m parse cast (l·∫•y 5 di·ªÖn vi√™n ƒë·∫ßu ti√™n)\n",
        "def extract_cast(cast_str):\n",
        "    try:\n",
        "        cast_list = json.loads(cast_str)\n",
        "        return ' '.join([c['name'].replace(' ', '') for c in cast_list[:5]])\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "# H√†m parse director t·ª´ crew\n",
        "def extract_director(crew_str):\n",
        "    try:\n",
        "        crew_list = json.loads(crew_str)\n",
        "        for person in crew_list:\n",
        "            if person.get('job') == 'Director':\n",
        "                return person['name'].replace(' ', '')\n",
        "        return ''\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "movies_merged['keywords_clean'] = movies_merged['keywords'].apply(extract_keywords)\n",
        "movies_merged['cast_clean'] = movies_merged['cast'].apply(extract_cast)\n",
        "movies_merged['director_clean'] = movies_merged['crew'].apply(extract_director)\n",
        "movies_merged['genres_clean'] = movies_merged['genres_list'].apply(lambda x: ' '.join([g.replace(' ', '') for g in x]))\n",
        "\n",
        "# K·∫øt h·ª£p c√°c features th√†nh m·ªôt chu·ªói duy nh·∫•t\n",
        "movies_merged['combined_features'] = (\n",
        "    movies_merged['overview'].fillna('') + ' ' +\n",
        "    movies_merged['genres_clean'] + ' ' +\n",
        "    movies_merged['keywords_clean'] + ' ' +\n",
        "    movies_merged['cast_clean'] + ' ' +\n",
        "    movies_merged['director_clean']\n",
        ")\n",
        "\n",
        "print(\"Sample combined features:\")\n",
        "print(movies_merged[['title', 'combined_features']].head(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.2. Vector h√≥a v·ªõi TF-IDF v√† t√≠nh Cosine Similarity\n",
        "\n",
        "# Kh·ªüi t·∫°o TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "# Fit v√† transform\n",
        "tfidf_matrix = tfidf.fit_transform(movies_merged['combined_features'])\n",
        "\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# T√≠nh cosine similarity\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(f\"Cosine similarity matrix shape: {cosine_sim.shape}\")\n",
        "\n",
        "# T·∫°o mapping t·ª´ title sang index\n",
        "indices = pd.Series(movies_merged.index, index=movies_merged['title']).drop_duplicates()\n",
        "\n",
        "print(f\"\\\\nS·ªë phim trong h·ªá th·ªëng: {len(indices)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.3. H√†m g·ª£i √Ω phim\n",
        "\n",
        "def get_recommendations(title, top_n=10):\n",
        "    \"\"\"\n",
        "    Tr·∫£ v·ªÅ top N phim t∆∞∆°ng t·ª± v·ªõi phim c√≥ title ƒë√£ cho\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # L·∫•y index c·ªßa phim\n",
        "        idx = indices[title]\n",
        "        \n",
        "        # L·∫•y similarity scores\n",
        "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "        \n",
        "        # S·∫Øp x·∫øp theo similarity (gi·∫£m d·∫ßn)\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # L·∫•y top N phim (b·ªè phim ƒë·∫ßu ti√™n v√¨ ch√≠nh n√≥)\n",
        "        sim_scores = sim_scores[1:top_n+1]\n",
        "        \n",
        "        # L·∫•y indices c·ªßa phim\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        \n",
        "        # Tr·∫£ v·ªÅ th√¥ng tin phim\n",
        "        result = movies_merged.iloc[movie_indices][['title', 'vote_average', 'vote_count', 'genres_clean', 'overview']]\n",
        "        result['similarity_score'] = [score[1] for score in sim_scores]\n",
        "        \n",
        "        return result\n",
        "    except KeyError:\n",
        "        return f\"Phim '{title}' kh√¥ng t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu.\"\n",
        "\n",
        "# Test h√†m g·ª£i √Ω\n",
        "print(\"=== G·ª£i √Ω phim t∆∞∆°ng t·ª± 'Avatar' ===\")\n",
        "recommendations = get_recommendations('Avatar', top_n=5)\n",
        "print(recommendations[['title', 'similarity_score', 'vote_average']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ƒê√°nh gi√° m√¥ h√¨nh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.1. ƒê√°nh gi√° RMSE v√† MAE\n",
        "\n",
        "# V·ªõi content-based filtering, ta ƒë√°nh gi√° b·∫±ng c√°ch:\n",
        "# - D·ª± ƒëo√°n rating c·ªßa phim g·ª£i √Ω = trung b√¨nh c√≥ tr·ªçng s·ªë theo similarity\n",
        "# - So s√°nh v·ªõi rating th·ª±c t·∫ø\n",
        "\n",
        "def predict_rating(title, top_n=10):\n",
        "    \"\"\"D·ª± ƒëo√°n rating cho phim d·ª±a tr√™n c√°c phim t∆∞∆°ng t·ª±\"\"\"\n",
        "    try:\n",
        "        idx = indices[title]\n",
        "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
        "        \n",
        "        # T√≠nh rating d·ª± ƒëo√°n = weighted average\n",
        "        total_sim = sum([score[1] for score in sim_scores])\n",
        "        if total_sim == 0:\n",
        "            return movies_merged.iloc[idx]['vote_average']\n",
        "        \n",
        "        weighted_rating = sum([\n",
        "            movies_merged.iloc[score[0]]['vote_average'] * score[1] \n",
        "            for score in sim_scores\n",
        "        ]) / total_sim\n",
        "        \n",
        "        return weighted_rating\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# L·∫•y sample ƒë·ªÉ ƒë√°nh gi√° (100 phim c√≥ vote_count > 100)\n",
        "sample_movies = movies_merged[movies_merged['vote_count'] > 100].sample(min(100, len(movies_merged)), random_state=42)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for title in sample_movies['title']:\n",
        "    true_rating = sample_movies[sample_movies['title'] == title]['vote_average'].values[0]\n",
        "    pred_rating = predict_rating(title, top_n=10)\n",
        "    \n",
        "    if pred_rating is not None:\n",
        "        y_true.append(true_rating)\n",
        "        y_pred.append(pred_rating)\n",
        "\n",
        "# T√≠nh RMSE v√† MAE\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"\\\\nS·ªë phim ƒë√°nh gi√°: {len(y_true)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.2. ƒê√°nh gi√° Precision@K v√† Recall@K\n",
        "\n",
        "# Gi·∫£ ƒë·ªãnh: Phim ƒë∆∞·ª£c coi l√† \"relevant\" n·∫øu c√≥ rating >= threshold v√† c√πng th·ªÉ lo·∫°i\n",
        "def evaluate_precision_recall(test_movies, K=10, rating_threshold=7.0):\n",
        "    \"\"\"\n",
        "    ƒê√°nh gi√° Precision@K v√† Recall@K\n",
        "    - Relevant items: phim c√≥ rating >= threshold v√† chia s·∫ª √≠t nh·∫•t 1 th·ªÉ lo·∫°i v·ªõi phim g·ªëc\n",
        "    \"\"\"\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    \n",
        "    for title in test_movies['title']:\n",
        "        movie_info = movies_merged[movies_merged['title'] == title].iloc[0]\n",
        "        movie_genres = set(movie_info['genres_list'])\n",
        "        \n",
        "        # L·∫•y recommendations\n",
        "        recs = get_recommendations(title, top_n=K)\n",
        "        \n",
        "        if isinstance(recs, str):  # Tr∆∞·ªùng h·ª£p kh√¥ng t√¨m th·∫•y\n",
        "            continue\n",
        "        \n",
        "        # T√¨m relevant items trong to√†n b·ªô dataset\n",
        "        relevant_items = movies_merged[\n",
        "            (movies_merged['vote_average'] >= rating_threshold) &\n",
        "            (movies_merged['title'] != title) &\n",
        "            (movies_merged['genres_list'].apply(lambda x: len(set(x) & movie_genres) > 0))\n",
        "        ]['title'].tolist()\n",
        "        \n",
        "        if len(relevant_items) == 0:\n",
        "            continue\n",
        "        \n",
        "        # T√≠nh s·ªë recommended items l√† relevant\n",
        "        recommended_titles = recs['title'].tolist()\n",
        "        relevant_recommended = set(recommended_titles) & set(relevant_items)\n",
        "        \n",
        "        # Precision@K = (relevant items in top K) / K\n",
        "        precision = len(relevant_recommended) / K\n",
        "        \n",
        "        # Recall@K = (relevant items in top K) / (total relevant items)\n",
        "        recall = len(relevant_recommended) / len(relevant_items) if len(relevant_items) > 0 else 0\n",
        "        \n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "    \n",
        "    return np.mean(precisions), np.mean(recalls)\n",
        "\n",
        "# ƒê√°nh gi√° tr√™n sample\n",
        "test_sample = movies_merged[movies_merged['vote_count'] > 50].sample(min(50, len(movies_merged)), random_state=42)\n",
        "precision_at_10, recall_at_10 = evaluate_precision_recall(test_sample, K=10, rating_threshold=7.0)\n",
        "\n",
        "print(f\"Precision@10: {precision_at_10:.4f}\")\n",
        "print(f\"Recall@10: {recall_at_10:.4f}\")\n",
        "print(f\"\\\\nS·ªë phim test: {len(test_sample)}\")\n",
        "print(f\"Ng∆∞·ª°ng rating cho relevant items: 7.0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.3. T√≥m t·∫Øt k·∫øt qu·∫£ ƒë√°nh gi√°\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"T·ªîNG K·∫æT K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úì Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"‚úì Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"‚úì Precision@10: {precision_at_10:.4f}\")\n",
        "print(f\"‚úì Recall@10: {recall_at_10:.4f}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\\\nM√¥ h√¨nh content-based filtering ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng th√†nh c√¥ng!\")\n",
        "print(f\"Dataset: {len(movies_merged)} phim t·ª´ TMDB 5000\")\n",
        "print(f\"Features: overview, genres, keywords, cast, director\")\n",
        "print(f\"Vectorization: TF-IDF (max_features=5000, ngram_range=(1,2))\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. L∆∞u model v√† d·ªØ li·ªáu ƒë·ªÉ tri·ªÉn khai\n",
        "\n",
        "import pickle\n",
        "\n",
        "# L∆∞u c√°c object c·∫ßn thi·∫øt\n",
        "data_to_save = {\n",
        "    'movies_data': movies_merged[['id', 'title', 'vote_average', 'vote_count', 'popularity', \n",
        "                                   'genres_clean', 'overview', 'release_date', 'runtime']],\n",
        "    'cosine_sim': cosine_sim,\n",
        "    'indices': indices\n",
        "}\n",
        "\n",
        "with open('movie_recommender_model.pkl', 'wb') as f:\n",
        "    pickle.dump(data_to_save, f)\n",
        "\n",
        "print(\"‚úì ƒê√£ l∆∞u model v√† d·ªØ li·ªáu v√†o file 'movie_recommender_model.pkl'\")\n",
        "print(f\"‚úì K√≠ch th∆∞·ªõc file: {os.path.getsize('movie_recommender_model.pkl') / (1024*1024):.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Web App\n",
        "\n",
        "ƒê·ªÉ ch·∫°y giao di·ªán web Streamlit:\n",
        "\n",
        "1. **ƒê·∫£m b·∫£o ƒë√£ ch·∫°y t·∫•t c·∫£ cell ·ªü tr√™n** ƒë·ªÉ t·∫°o file `movie_recommender_model.pkl`\n",
        "\n",
        "2. **M·ªü terminal/command prompt** v√† ch·∫°y l·ªánh:\n",
        "   ```\n",
        "   streamlit run app.py\n",
        "   ```\n",
        "\n",
        "3. **Tr√¨nh duy·ªát s·∫Ω t·ª± ƒë·ªông m·ªü** t·∫°i `http://localhost:8501`\n",
        "\n",
        "4. **S·ª≠ d·ª•ng app**:\n",
        "   - Ch·ªçn phim y√™u th√≠ch t·ª´ dropdown\n",
        "   - ƒêi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng g·ª£i √Ω\n",
        "   - Nh·∫•n n√∫t \"T√¨m phim t∆∞∆°ng t·ª±\"\n",
        "   - Xem k·∫øt qu·∫£ v√† download CSV n·∫øu c·∫ßn\n",
        "\n",
        "---\n",
        "\n",
        "### üéâ Ho√†n th√†nh!\n",
        "\n",
        "Project ƒë√£ bao g·ªìm:\n",
        "- ‚úÖ Notebook ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß (`tmdb_recommender.ipynb`)\n",
        "- ‚úÖ Web App v·ªõi Streamlit (`app.py`)\n",
        "- ‚úÖ Requirements file (`requirements.txt`)\n",
        "- ‚úÖ H∆∞·ªõng d·∫´n chi ti·∫øt (`README.md`)\n",
        "- ‚úÖ Model ƒë√£ l∆∞u (`movie_recommender_model.pkl`)\n",
        "\n",
        "Ch√∫c b·∫°n ho√†n th√†nh t·ªët Final Project! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
